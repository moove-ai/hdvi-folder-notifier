import base64
import json
import os
import logging
import threading
import time
from datetime import datetime
from typing import Dict, Tuple
from flask import Flask, request, jsonify
import requests
from google.cloud import firestore
from google.cloud import storage

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# Configuration
PROJECT_ID = os.environ.get("GCP_PROJECT", "moove-data-pipelines")
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL", "")
SLACK_BOT_TOKEN = os.environ.get("SLACK_BOT_TOKEN", "")
SLACK_CHANNEL = os.environ.get("SLACK_CHANNEL", "")
BUCKET_NAME = os.environ.get("BUCKET_NAME", "moove-incoming-data-u7x4ty")
MONITORED_PREFIXES = os.environ.get("MONITORED_PREFIXES", "Prebind/,Postbind/,test/").split(",")

# Optional analytics CSV sink in GCS
ANALYTICS_BUCKET = os.environ.get("ANALYTICS_BUCKET", "")
ANALYTICS_OBJECT = os.environ.get("ANALYTICS_OBJECT", "")

# Initialize Firestore to track notified folders
db = firestore.Client(project=PROJECT_ID)
COLLECTION_NAME = "notified_folders"

# Initialize GCS client
storage_client = storage.Client(project=PROJECT_ID)
bucket_client = storage_client.bucket(BUCKET_NAME)

# Folder monitoring state
# Maps folder_path -> {"last_update": datetime, "known_files": set, "monitoring_thread": Thread}
monitored_folders: Dict[str, Dict] = {}
monitored_folders_lock = threading.Lock()

# Monitoring configuration
CHECK_INTERVAL_SECONDS = 15
INACTIVITY_TIMEOUT_SECONDS = 60


def _update_slack_metadata_with_retry(doc_id: str, ts: str, channel: str, retries: int = 3) -> None:
    """Best-effort save of Slack message identifiers to Firestore to enable later edits."""
    last_err = None
    for _ in range(retries):
        try:
            doc_ref = db.collection(COLLECTION_NAME).document(doc_id)
            doc_ref.update({
                "slack_message_ts": ts,
                "slack_channel": channel,
            })
            return
        except Exception as e:
            last_err = e
            time.sleep(0.2)
    if last_err:
        logger.error(f"Failed to save Slack message metadata after retries for doc {doc_id}: {last_err}")


def get_folder_from_path(file_path: str) -> str:
    """
    Extract the specific subfolder from a file path within monitored prefixes.
    Only returns a subfolder if there actually is one.
    Example: test/subfolder/file.csv -> test/subfolder
    Example: test/file.csv -> test (no subfolder)
    """
    # Find which monitored prefix this file belongs to
    for prefix in MONITORED_PREFIXES:
        if file_path.startswith(prefix):
            # Remove the prefix and get the next path component (subfolder)
            # Normalize to ensure we don't create double slashes later
            norm_prefix = prefix.rstrip('/')
            relative_path = file_path[len(prefix):].lstrip('/')
            if relative_path:
                # Get the first path component after the prefix
                subfolder = relative_path.split('/')[0]
                # Only return subfolder if it's not a file (has no extension or is a directory)
                if '.' not in subfolder or subfolder.count('/') > 0:
                    return f"{norm_prefix}/{subfolder.strip('/')}"
            # If no subfolder or it's a file directly in the prefix, return just the prefix
            return norm_prefix
    return ""


def is_monitored_path(file_path: str) -> bool:
    """Check if the file path starts with one of the monitored prefixes."""
    return any(file_path.startswith(prefix) for prefix in MONITORED_PREFIXES)


@firestore.transactional
def check_and_mark_folder(transaction, folder_path: str, timestamp: str) -> bool:
    """
    Atomically check if folder has been notified and mark it if not.
    Returns True if this is a new folder (should notify), False otherwise.
    """
    # Encode folder path to make it a valid Firestore document ID
    doc_id = folder_path.replace("/", "_").replace("\\", "_")
    doc_ref = db.collection(COLLECTION_NAME).document(doc_id)
    doc = doc_ref.get(transaction=transaction)
    
    if doc.exists:
        return False  # Already notified
    
    # Mark as notified within the same transaction
    transaction.set(
        doc_ref,
        {
            "folder_path": folder_path,
            "doc_id": doc_id,
            "first_notification_time": timestamp,
            "notified_at": firestore.SERVER_TIMESTAMP,
            # Slack message linkage (optional, when using Slack Web API)
            "slack_message_ts": None,
            "slack_channel": SLACK_CHANNEL or None,
        },
    )
    return True  # New folder, should notify


@firestore.transactional
def check_and_mark_final(transaction, folder_path: str, file_count: int, total_size: int) -> bool:
    """
    Atomically check if final notification was already sent; if not, mark it with stats.
    Returns True if we should send final notification/edit now, False otherwise.
    """
    doc_id = folder_path.replace("/", "_").replace("\\", "_")
    doc_ref = db.collection(COLLECTION_NAME).document(doc_id)
    doc = doc_ref.get(transaction=transaction)
    data = doc.to_dict() or {}
    if doc.exists and data.get("final_notification_sent"):
        return False

    # Mark final as sent and store stats
    transaction.set(
        doc_ref,
        {
            "folder_path": folder_path,
            "doc_id": doc_id,
            "final_notification_sent": True,
            "final_notification_time": firestore.SERVER_TIMESTAMP,
            "file_count": file_count,
            "total_size_bytes": total_size,
        },
        merge=True,
    )
    return True


def _slack_api_post(path: str, payload: Dict) -> Dict:
    url = f"https://slack.com/api/{path}"
    headers = {
        "Authorization": f"Bearer {SLACK_BOT_TOKEN}",
        "Content-Type": "application/json;charset=utf-8",
    }
    resp = requests.post(url, headers=headers, json=payload, timeout=10)
    resp.raise_for_status()
    data = resp.json()
    if not data.get("ok"):
        raise RuntimeError(f"Slack API error for {path}: {data}")
    return data


def send_slack_notification(folder_path: str, timestamp: str) -> bool:
    """Send initial notification to Slack. If bot token/channel are set, use chat.postMessage and store ts; else fallback to webhook."""
    blocks = [
        {
            "type": "header",
            "text": {"type": "plain_text", "text": "📁 New HDVI Data Folder"},
        },
        {
            "type": "section",
            "fields": [
                {"type": "mrkdwn", "text": f"*Folder:*\n`{BUCKET_NAME}/{folder_path}`"},
                {"type": "mrkdwn", "text": f"*First File Time:*\n{timestamp}"},
            ],
        },
    ]

    # Prefer Slack Web API when configured
    if SLACK_BOT_TOKEN and SLACK_CHANNEL:
        try:
            res = _slack_api_post(
                "chat.postMessage",
                {
                    "channel": SLACK_CHANNEL,
                    "text": f"New folder: {BUCKET_NAME}/{folder_path}",
                    "blocks": blocks,
                },
            )
            ts = res.get("ts")
            channel = res.get("channel") or SLACK_CHANNEL
            doc_id = folder_path.replace("/", "_").replace("\\", "_")
            if ts and channel:
                _update_slack_metadata_with_retry(doc_id, ts, channel)
            logger.info(f"Slack message posted with ts={ts} channel={channel} for folder: {folder_path}")
            return True
        except Exception as e:
            # Do NOT fallback to webhook when bot token is configured; avoid duplicate messages
            logger.error(f"Failed Slack Web API post: {e}")
            return False

    # Fallback to webhook (cannot edit later) only when bot token not configured
    if SLACK_BOT_TOKEN:
        # Bot is configured but post failed above; do not send webhook fallback
        return False
    if not SLACK_WEBHOOK_URL:
        logger.warning("SLACK_WEBHOOK_URL not configured, skipping webhook notification")
        return False

    message = {"text": f"🆕 New folder detected in HDVI data", "blocks": blocks}
    try:
        response = requests.post(SLACK_WEBHOOK_URL, json=message, timeout=10)
        response.raise_for_status()
        logger.info(f"Slack notification sent for folder: {folder_path}")
        return True
    except Exception as e:
        logger.error(f"Failed to send Slack notification via webhook: {e}")
        return False


def get_folder_stats(folder_path: str) -> Tuple[int, int]:
    """
    Get statistics for a folder.
    Returns: (count of jsonl.gz files, total size in bytes)
    """
    try:
        # List all blobs with the folder prefix (use client-level listing for robustness)
        prefix = f"{folder_path}/" if not folder_path.endswith("/") else folder_path
        logger.info(f"Listing blobs for stats: bucket={BUCKET_NAME} prefix={prefix}")
        blobs = storage_client.list_blobs(BUCKET_NAME, prefix=prefix)

        jsonl_gz_count = 0
        total_size = 0
        scanned = 0

        for blob in blobs:
            scanned += 1
            # Count any files anywhere under the folder (including subfolders)
            if not blob.name.endswith('/') and blob.name.endswith('.jsonl.gz'):
                jsonl_gz_count += 1
                total_size += blob.size
        logger.info(f"Folder stats listing complete: scanned={scanned} matched_jsonl_gz={jsonl_gz_count} total_size={total_size}")
        
        return jsonl_gz_count, total_size
    except Exception as e:
        logger.error(f"Error getting folder stats for {folder_path}: {e}")
        return 0, 0


def format_size(size_bytes: int) -> str:
    """Format size in human-readable format."""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f} PB"


def send_final_slack_notification(folder_path: str, file_count: int, total_size: int) -> bool:
    """Edit the original Slack message with final statistics when possible; else send a second message via webhook."""
    size_str = format_size(total_size)
    final_blocks = [
        {
            "type": "header",
            "text": {"type": "plain_text", "text": "✅ HDVI Folder Upload Complete"},
        },
        {
            "type": "section",
            "fields": [
                {"type": "mrkdwn", "text": f"*Folder:*\n`{BUCKET_NAME}/{folder_path}`"},
                {"type": "mrkdwn", "text": f"*JSONL.GZ Files:*\n{file_count}"},
                {"type": "mrkdwn", "text": f"*Total Size:*\n{size_str}"},
            ],
        },
    ]

    # Prefer editing the original message when token/channel + ts exist
    if SLACK_BOT_TOKEN:
        try:
            doc_id = folder_path.replace("/", "_").replace("\\", "_")
            doc_ref = db.collection(COLLECTION_NAME).document(doc_id)
            doc = doc_ref.get()
            data = doc.to_dict() or {}
            ts = (doc.exists and data.get("slack_message_ts")) or None
            channel = (doc.exists and data.get("slack_channel")) or SLACK_CHANNEL
            logger.info(f"Preparing Slack edit: doc_exists={doc.exists} ts={ts} channel={channel} doc_id={doc_id}")
            if ts and channel:
                _slack_api_post(
                    "chat.update",
                    {
                        "channel": channel,
                        "ts": ts,
                        "text": f"Folder complete: {BUCKET_NAME}/{folder_path}",
                        "blocks": final_blocks,
                    },
                )
                logger.info(f"Edited Slack message ts={ts} channel={channel} for folder: {folder_path} with file_count={file_count} total_size={total_size}")
                return True
            else:
                logger.warning(f"Cannot edit Slack message: missing ts/channel for folder {folder_path}")
        except Exception as e:
            # Do NOT fallback to webhook when bot token is configured; avoid double messages
            logger.error(f"Failed to edit Slack message: {e}")
            return False

    # Fallback: only when no bot token configured
    if not SLACK_BOT_TOKEN and SLACK_WEBHOOK_URL:
        try:
            message = {"text": f"✅ Folder upload complete: {folder_path}", "blocks": final_blocks}
            response = requests.post(SLACK_WEBHOOK_URL, json=message, timeout=10)
            response.raise_for_status()
            logger.info(f"Final Slack notification sent (webhook) for folder: {folder_path} with file_count={file_count} total_size={total_size}")
            return True
        except Exception as e:
            logger.error(f"Failed to send final Slack notification via webhook: {e}")
            return False

    logger.warning("No Slack mechanism configured for final notification")
    return False


def _append_completion_csv(folder_path: str, first_time: str, final_time_iso: str, file_count: int, total_size: int) -> None:
    """Append a CSV row to GCS object with folder completion stats. Best-effort with generation precondition retries."""
    if not ANALYTICS_BUCKET or not ANALYTICS_OBJECT:
        return

    try:
        import csv
        from io import StringIO

        bucket = storage_client.bucket(ANALYTICS_BUCKET)
        blob = bucket.blob(ANALYTICS_OBJECT)

        # Build row via csv writer
        buf = StringIO()
        writer = csv.writer(buf)
        writer.writerow([folder_path, first_time or "", final_time_iso, str(file_count), str(total_size)])
        row_bytes = buf.getvalue()

        # If object does not exist, write header + row
        if not blob.exists():
            header_buf = StringIO()
            writer_h = csv.writer(header_buf)
            writer_h.writerow(["folder_path", "first_notification_time", "final_notification_time", "file_count", "total_size_bytes"])
            data = header_buf.getvalue() + row_bytes
            blob.upload_from_string(data, content_type="text/csv")
            logger.info(f"Created analytics CSV with first row for {folder_path}")
            return

        # Else: read-modify-write with generation precondition
        retries = 3
        for attempt in range(retries):
            blob.reload()
            gen = blob.generation
            existing = blob.download_as_text()
            new_data = existing + row_bytes
            try:
                blob.upload_from_string(new_data, content_type="text/csv", if_generation_match=gen)
                logger.info(f"Appended analytics CSV row for {folder_path}")
                return
            except Exception as e:
                if attempt < retries - 1:
                    time.sleep(0.2)
                    continue
                logger.error(f"Failed to append analytics CSV for {folder_path}: {e}")
                return
    except Exception as e:
        logger.error(f"Analytics CSV error for {folder_path}: {e}")


def check_folder_for_new_files(folder_path: str) -> bool:
    """
    Check if there are new files in the folder since last check.
    Returns True if new files were found, False otherwise.
    """
    try:
        prefix = f"{folder_path}/" if not folder_path.endswith("/") else folder_path
        logger.debug(f"Checking folder for new files: bucket={BUCKET_NAME} prefix={prefix}")
        blobs = storage_client.list_blobs(BUCKET_NAME, prefix=prefix)
        
        with monitored_folders_lock:
            if folder_path not in monitored_folders:
                return False
            
            known_files = monitored_folders[folder_path]["known_files"]
            found_new = False
            
            scanned = 0
            for blob in blobs:
                scanned += 1
                # Consider any non-directory blob under the prefix
                if not blob.name.endswith('/'):
                    if blob.name not in known_files:
                        known_files.add(blob.name)
                        found_new = True
                        logger.debug(f"New file detected in {folder_path}: {blob.name}")
            
            if found_new:
                monitored_folders[folder_path]["last_update"] = datetime.utcnow()
            else:
                logger.debug(f"No new files found. scanned={scanned} known_files={len(known_files)}")
            
            return found_new
    except Exception as e:
        logger.error(f"Error checking folder {folder_path} for new files: {e}")
        return False


def monitor_folder(folder_path: str):
    """
    Monitor a folder at 15-second intervals.
    After 1 minute of inactivity, send final notification and stop monitoring.
    """
    logger.info(f"Starting monitoring for folder: {folder_path}")
    
    try:
        while True:
            time.sleep(CHECK_INTERVAL_SECONDS)
            
            with monitored_folders_lock:
                if folder_path not in monitored_folders:
                    logger.info(f"Folder {folder_path} removed from monitoring")
                    break
                
                folder_state = monitored_folders[folder_path]
                last_update = folder_state["last_update"]
            
            # Check for new files
            found_new = check_folder_for_new_files(folder_path)
            
            if found_new:
                logger.debug(f"New files found in {folder_path}, continuing monitoring")
                continue
            
            # Check if we've passed the inactivity timeout
            now = datetime.utcnow()
            time_since_last_update = (now - last_update).total_seconds()
            
            if time_since_last_update >= INACTIVITY_TIMEOUT_SECONDS:
                logger.info(f"No new files in {folder_path} for {INACTIVITY_TIMEOUT_SECONDS}s, preparing final notification")
                
                # Get folder statistics
                file_count, total_size = get_folder_stats(folder_path)
                
                # Idempotent final-send gate using Firestore
                try:
                    transaction = db.transaction()
                    should_send_final = check_and_mark_final(transaction, folder_path, file_count, total_size)
                except Exception as e:
                    logger.error(f"Error checking/marking final notification for {folder_path}: {e}")
                    should_send_final = False

                if should_send_final:
                    # Send final notification (edit or webhook depending on config)
                    send_final_slack_notification(folder_path, file_count, total_size)
                    # Write analytics CSV (best-effort)
                    try:
                        # Retrieve first notification time from Firestore for row
                        doc_id = folder_path.replace("/", "_").replace("\\", "_")
                        doc_ref = db.collection(COLLECTION_NAME).document(doc_id)
                        doc = doc_ref.get()
                        data = doc.to_dict() or {}
                        first_time = data.get("first_notification_time") or ""
                        final_time_iso = datetime.utcnow().isoformat()
                        _append_completion_csv(folder_path, first_time, final_time_iso, file_count, total_size)
                    except Exception as e:
                        logger.error(f"Failed to write analytics CSV for {folder_path}: {e}")
                
                # Else: another instance already sent the final, skip sending
                
                # Remove from monitoring
                with monitored_folders_lock:
                    monitored_folders.pop(folder_path, None)
                
                logger.info(f"Stopped monitoring folder: {folder_path}")
                break
                
    except Exception as e:
        logger.error(f"Error in monitoring thread for {folder_path}: {e}", exc_info=True)
        with monitored_folders_lock:
            monitored_folders.pop(folder_path, None)


def start_folder_monitoring(folder_path: str, initial_file: str):
    """
    Start monitoring a folder in a background thread.
    """
    with monitored_folders_lock:
        if folder_path in monitored_folders:
            # Already monitoring, just update the last update time
            monitored_folders[folder_path]["last_update"] = datetime.utcnow()
            monitored_folders[folder_path]["known_files"].add(initial_file)
            logger.debug(f"Updated monitoring for existing folder: {folder_path}")
            return
        
        # Start new monitoring
        folder_state = {
            "last_update": datetime.utcnow(),
            "known_files": {initial_file},
            "monitoring_thread": None,
        }
        monitored_folders[folder_path] = folder_state
        
        # Start monitoring thread
        thread = threading.Thread(
            target=monitor_folder,
            args=(folder_path,),
            daemon=True,
            name=f"monitor-{folder_path}"
        )
        thread.start()
        folder_state["monitoring_thread"] = thread
        
        logger.info(f"Started monitoring thread for folder: {folder_path}")


@app.route("/", methods=["POST"])
def handle_pubsub_push():
    """Handle Pub/Sub push messages."""
    try:
        envelope = request.get_json()
        if not envelope:
            logger.warning("No Pub/Sub message received")
            return "Bad Request: no Pub/Sub message received", 400

        if not isinstance(envelope, dict) or "message" not in envelope:
            logger.warning("Invalid Pub/Sub message format")
            return "Bad Request: invalid Pub/Sub message format", 400

        pubsub_message = envelope["message"]

        # Decode the message data
        if "data" in pubsub_message:
            message_data = base64.b64decode(pubsub_message["data"]).decode("utf-8")
            logger.info(f"Received message: {message_data}")

            try:
                data = json.loads(message_data)
            except json.JSONDecodeError:
                logger.error(f"Failed to parse message data as JSON: {message_data}")
                return "OK", 200  # Still return OK to acknowledge the message

            # Extract file information from GCS notification
            file_name = data.get("name", "")
            bucket = data.get("bucket", "")
            event_time = data.get("timeCreated", datetime.utcnow().isoformat())

            logger.info(f"Processing file: {bucket}/{file_name}")

            # Check if this is in a monitored path
            if bucket == BUCKET_NAME and is_monitored_path(file_name):
                folder_path = get_folder_from_path(file_name)

                if folder_path:
                    # Atomically check and mark folder (prevents race conditions)
                    transaction = db.transaction()
                    should_notify = check_and_mark_folder(transaction, folder_path, event_time)

                    if should_notify:
                        logger.info(f"New folder detected: {folder_path}")
                        # Send Slack notification
                        if send_slack_notification(folder_path, event_time):
                            logger.info(f"Notification sent for folder: {folder_path}")
                        else:
                            logger.warning(f"Failed to send Slack notification for folder: {folder_path}")
                            # Note: Folder is still marked as notified in Firestore to prevent retries
                        
                        # Start monitoring this folder
                        start_folder_monitoring(folder_path, file_name)
                    else:
                        logger.debug(f"Folder already notified: {folder_path}")
                        # Even if already notified, we might want to track this file for monitoring
                        # Check if we're still monitoring this folder
                        with monitored_folders_lock:
                            if folder_path in monitored_folders:
                                # Update monitoring with this new file
                                monitored_folders[folder_path]["last_update"] = datetime.utcnow()
                                monitored_folders[folder_path]["known_files"].add(file_name)
                            else:
                                # Folder was already notified but monitoring completed or never started
                                # Check Firestore to see if final notification was sent
                                doc_id = folder_path.replace("/", "_").replace("\\", "_")
                                doc_ref = db.collection(COLLECTION_NAME).document(doc_id)
                                doc = doc_ref.get()
                                data = doc.to_dict() or {}
                                if doc.exists and not data.get("final_notification_sent"):
                                    # Final notification not sent yet, start monitoring
                                    start_folder_monitoring(folder_path, file_name)
                else:
                    logger.debug(f"Empty folder path for file: {file_name}")
            else:
                logger.debug(f"File not in monitored path: {file_name}")

        return "OK", 200

    except Exception as e:
        logger.error(f"Error processing message: {e}", exc_info=True)
        return "Internal Server Error", 500


@app.route("/health", methods=["GET"])
def health_check():
    """Health check endpoint."""
    return jsonify({"status": "healthy"}), 200


@app.route("/_ah/warmup", methods=["GET"])
def warmup():
    """Warmup endpoint for Cloud Run."""
    return "OK", 200


if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8080))
    app.run(host="0.0.0.0", port=port, debug=False)

